{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "DiKnfq_meB3x"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import librosa\n",
        "import librosa.display\n",
        "import torch\n",
        "from torch import nn\n",
        "from torchvision import models, transforms, datasets\n",
        "import torch.utils.data as data\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from torchvision.models import VGG16_Weights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "NrLKx7TNeSOZ"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu' #checking for gpu\n",
        "\n",
        "reverse_genredict = {0:\"blues\", 1:\"classical\", 2:\"country\", 3:\"disco\", 4:\"hiphop\", 5:\"jazz\", 6:\"metal\", 7:\"pop\", 8:\"reggae\",9:\"rock\"}\n",
        "genre_array = [\"blues\",\"classical\",\"country\",\"disco\",\"hiphop\",\"jazz\",\"metal\",\"pop\",\"reggae\",\"rock\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8V33z6vfeV2g",
        "outputId": "ca37e005-ca64-4500-c9a9-7d434690dfbd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "7SLKn0L4eeeu"
      },
      "outputs": [],
      "source": [
        "#defining custom dataset to input to model\n",
        "class MusicGenres(data.Dataset):\n",
        "  def __init__(self, csv_file, root_dir, transform=None):\n",
        "    self.annotations = pd.read_csv(csv_file)\n",
        "    self.root_dir = root_dir\n",
        "    self.transform = transform\n",
        "    self.genre_dict = {\"blues\":0,\"classical\":1,\"country\":2,\"disco\":3,\"hiphop\":4,\"jazz\":5,\"metal\":6,\"pop\":7,\"reggae\":8,\"rock\":9}\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.annotations)\n",
        "\n",
        "  def __getitem__(self,index):\n",
        "    name = str(self.annotations.iloc[index,0])\n",
        "    y_label = self.genre_dict[str(self.annotations.iloc[index,59])]\n",
        "    im = torch.load(self.root_dir+name+'.pt') #loading saved torch tensor representing file\n",
        "    im = torch.unsqueeze(im,axis=0)  #adding 3rd dimension\n",
        "    im = torch.tile(im,(3,1,1)) #repeating 2 dimensional array across 3 dimensions to resemble image input\n",
        "    if self.transform:\n",
        "      im = self.transform(im)\n",
        "\n",
        "    return (im, y_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "NJQ3SYLvej8W"
      },
      "outputs": [],
      "source": [
        "path_tensors = \"drive/Shareddrives/DL Final Project/spectrogram_tensors/\"\n",
        "csv_file = 'drive/Shareddrives/DL Final Project/DL Final Project/features_3_sec.csv'\n",
        "#splitting dataset into train and test\n",
        "dataset = MusicGenres(csv_file = csv_file, root_dir = path_tensors)\n",
        "length = len(dataset)\n",
        "split = 0.15\n",
        "test_size = int(length*split)\n",
        "train_size = length - test_size\n",
        "traindata, testdata = data.random_split(dataset, [train_size, test_size])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cryeP4eceqp1"
      },
      "outputs": [],
      "source": [
        "class MusicClassifier(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(MusicClassifier, self).__init__()\n",
        "    self.sequential = nn.Sequential(\n",
        "        nn.Conv2d(1,1,3),\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(12032,1024),\n",
        "        nn.Linear(1024,10),\n",
        "        nn.Softmax()\n",
        "    )\n",
        "  \n",
        "  def forward(self,x):\n",
        "    probabilities = self.sequential(x)\n",
        "    return probabilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "u34cNtVSetZp"
      },
      "outputs": [],
      "source": [
        "#saves model weights\n",
        "def saveModel(model):\n",
        "    path = \"drive/Shareddrives/DL Final Project/DL Final Project/model.pth\"\n",
        "    torch.save(model.state_dict(), path)\n",
        "\n",
        "# Function to test the model with the test dataset and print the accuracy for the test images\n",
        "def testAccuracy(model, test_loader):\n",
        "\n",
        "    model.to(device)  \n",
        "    model.eval()\n",
        "    accuracy = 0.0\n",
        "    total = 0.0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for data in test_loader:\n",
        "\n",
        "            images, labels = data\n",
        "            images = Variable(images.to(device))\n",
        "            labels = Variable(labels.to(device))\n",
        "            # run the model on the test set to predict labels\n",
        "            outputs = model(images)\n",
        "            # the label with the highest probability will be our prediction\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            accuracy += (predicted == labels).sum().item()\n",
        "    \n",
        "    # compute the accuracy over all test images\n",
        "    accuracy = (100 * accuracy / total)\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "# Training function. We simply have to loop over our data iterator and feed the inputs to the network and optimize.\n",
        "def train(model, num_epochs, loss_fn, train_loader, test_loader, optimizer):\n",
        "    print(\"The model will be running on\", device, \"device\")\n",
        "    # Convert model parameters and buffers to CPU or Cuda\n",
        "    model.to(device)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        size = len(train_loader.dataset)\n",
        "\n",
        "        for batch, (images, labels) in enumerate(train_loader, 0):\n",
        "            \n",
        "            # get the inputs\n",
        "            images = Variable(images.to(device))\n",
        "            labels = Variable(labels.to(device))\n",
        "            \n",
        "            pred = model(images)\n",
        "            loss = loss_fn(pred, labels)\n",
        "\n",
        "           # Backpropagation\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        " \n",
        "            loss, current = loss.item(), batch * len(images)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "        accuracy = testAccuracy(model, test_loader)\n",
        "        print('For epoch', epoch+1,'the test accuracy over the entire test set is %d %%' % (accuracy))\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYrTRjjAhOOl",
        "outputId": "6f694063-3e20-4cb4-9067-bedf14329594"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model will be running on cuda:0 device\n",
            "loss: 8.808786  [    0/ 8483]\n",
            "loss: 8.310627  [  200/ 8483]\n",
            "loss: 3.234228  [  400/ 8483]\n",
            "loss: 2.341087  [  600/ 8483]\n",
            "loss: 2.884772  [  800/ 8483]\n",
            "loss: 3.906440  [ 1000/ 8483]\n",
            "loss: 2.587358  [ 1200/ 8483]\n",
            "loss: 2.283644  [ 1400/ 8483]\n",
            "loss: 3.540798  [ 1600/ 8483]\n",
            "loss: 2.473015  [ 1800/ 8483]\n",
            "loss: 2.331269  [ 2000/ 8483]\n",
            "loss: 2.315682  [ 2200/ 8483]\n",
            "loss: 2.340773  [ 2400/ 8483]\n",
            "loss: 2.320840  [ 2600/ 8483]\n",
            "loss: 2.329386  [ 2800/ 8483]\n",
            "loss: 2.303628  [ 3000/ 8483]\n",
            "loss: 2.608539  [ 3200/ 8483]\n",
            "loss: 2.317037  [ 3400/ 8483]\n",
            "loss: 2.401185  [ 3600/ 8483]\n",
            "loss: 2.307696  [ 3800/ 8483]\n",
            "loss: 2.302832  [ 4000/ 8483]\n",
            "loss: 2.310713  [ 4200/ 8483]\n",
            "loss: 2.420046  [ 4400/ 8483]\n",
            "loss: 2.315283  [ 4600/ 8483]\n",
            "loss: 2.301566  [ 4800/ 8483]\n",
            "loss: 2.350875  [ 5000/ 8483]\n",
            "loss: 2.317439  [ 5200/ 8483]\n",
            "loss: 2.311622  [ 5400/ 8483]\n",
            "loss: 2.311143  [ 5600/ 8483]\n",
            "loss: 2.301203  [ 5800/ 8483]\n",
            "loss: 2.304412  [ 6000/ 8483]\n",
            "loss: 2.304353  [ 6200/ 8483]\n",
            "loss: 2.296914  [ 6400/ 8483]\n",
            "loss: 7.786094  [ 6600/ 8483]\n",
            "loss: 2.458470  [ 6800/ 8483]\n",
            "loss: 2.477441  [ 7000/ 8483]\n",
            "loss: 2.383557  [ 7200/ 8483]\n",
            "loss: 2.362817  [ 7400/ 8483]\n",
            "loss: 2.284778  [ 7600/ 8483]\n",
            "loss: 2.323079  [ 7800/ 8483]\n",
            "loss: 2.297549  [ 8000/ 8483]\n",
            "loss: 2.290565  [ 8200/ 8483]\n",
            "loss: 2.270066  [ 3486/ 8483]\n",
            "For epoch 1 the test accuracy over a batch of the test set is 9 %\n",
            "loss: 2.309175  [    0/ 8483]\n",
            "loss: 2.332068  [  200/ 8483]\n",
            "loss: 2.297374  [  400/ 8483]\n",
            "loss: 2.303335  [  600/ 8483]\n",
            "loss: 2.297932  [  800/ 8483]\n",
            "loss: 2.300287  [ 1000/ 8483]\n",
            "loss: 2.301004  [ 1200/ 8483]\n",
            "loss: 2.303751  [ 1400/ 8483]\n",
            "loss: 2.290012  [ 1600/ 8483]\n",
            "loss: 2.307622  [ 1800/ 8483]\n",
            "loss: 2.291331  [ 2000/ 8483]\n",
            "loss: 2.315253  [ 2200/ 8483]\n",
            "loss: 2.296873  [ 2400/ 8483]\n",
            "loss: 2.295290  [ 2600/ 8483]\n",
            "loss: 2.300067  [ 2800/ 8483]\n",
            "loss: 2.298404  [ 3000/ 8483]\n",
            "loss: 2.292131  [ 3200/ 8483]\n",
            "loss: 2.291691  [ 3400/ 8483]\n",
            "loss: 2.291911  [ 3600/ 8483]\n",
            "loss: 2.335562  [ 3800/ 8483]\n",
            "loss: 2.298031  [ 4000/ 8483]\n",
            "loss: 2.297462  [ 4200/ 8483]\n",
            "loss: 2.295216  [ 4400/ 8483]\n",
            "loss: 2.301732  [ 4600/ 8483]\n",
            "loss: 2.291965  [ 4800/ 8483]\n",
            "loss: 2.296300  [ 5000/ 8483]\n",
            "loss: 2.311003  [ 5200/ 8483]\n",
            "loss: 2.293542  [ 5400/ 8483]\n",
            "loss: 2.299477  [ 5600/ 8483]\n",
            "loss: 2.298784  [ 5800/ 8483]\n",
            "loss: 2.288836  [ 6000/ 8483]\n",
            "loss: 2.299875  [ 6200/ 8483]\n",
            "loss: 2.293550  [ 6400/ 8483]\n",
            "loss: 2.292983  [ 6600/ 8483]\n",
            "loss: 2.297915  [ 6800/ 8483]\n",
            "loss: 2.287795  [ 7000/ 8483]\n",
            "loss: 2.351968  [ 7200/ 8483]\n",
            "loss: 2.282340  [ 7400/ 8483]\n",
            "loss: 2.284118  [ 7600/ 8483]\n",
            "loss: 2.289617  [ 7800/ 8483]\n",
            "loss: 2.290318  [ 8000/ 8483]\n",
            "loss: 2.286348  [ 8200/ 8483]\n",
            "loss: 2.293875  [ 3486/ 8483]\n",
            "For epoch 2 the test accuracy over a batch of the test set is 11 %\n",
            "loss: 2.286119  [    0/ 8483]\n",
            "loss: 2.319437  [  200/ 8483]\n",
            "loss: 2.288698  [  400/ 8483]\n",
            "loss: 2.290210  [  600/ 8483]\n",
            "loss: 2.340689  [  800/ 8483]\n",
            "loss: 2.276133  [ 1000/ 8483]\n",
            "loss: 2.284381  [ 1200/ 8483]\n",
            "loss: 2.294492  [ 1400/ 8483]\n",
            "loss: 2.297832  [ 1600/ 8483]\n",
            "loss: 2.293415  [ 1800/ 8483]\n",
            "loss: 2.315592  [ 2000/ 8483]\n",
            "loss: 2.285279  [ 2200/ 8483]\n",
            "loss: 2.291474  [ 2400/ 8483]\n",
            "loss: 2.291369  [ 2600/ 8483]\n",
            "loss: 2.295885  [ 2800/ 8483]\n",
            "loss: 2.285782  [ 3000/ 8483]\n",
            "loss: 2.286152  [ 3200/ 8483]\n",
            "loss: 2.290439  [ 3400/ 8483]\n",
            "loss: 2.272640  [ 3600/ 8483]\n",
            "loss: 2.267638  [ 3800/ 8483]\n",
            "loss: 2.269804  [ 4000/ 8483]\n",
            "loss: 2.256836  [ 4200/ 8483]\n",
            "loss: 2.375090  [ 4400/ 8483]\n",
            "loss: 2.280137  [ 4600/ 8483]\n",
            "loss: 2.297539  [ 4800/ 8483]\n",
            "loss: 2.311349  [ 5000/ 8483]\n",
            "loss: 2.295113  [ 5200/ 8483]\n",
            "loss: 2.294180  [ 5400/ 8483]\n",
            "loss: 2.293438  [ 5600/ 8483]\n",
            "loss: 2.287706  [ 5800/ 8483]\n",
            "loss: 2.293390  [ 6000/ 8483]\n",
            "loss: 2.276111  [ 6200/ 8483]\n",
            "loss: 2.304292  [ 6400/ 8483]\n",
            "loss: 2.288987  [ 6600/ 8483]\n",
            "loss: 2.291485  [ 6800/ 8483]\n",
            "loss: 2.284106  [ 7000/ 8483]\n",
            "loss: 2.276666  [ 7200/ 8483]\n",
            "loss: 2.291991  [ 7400/ 8483]\n",
            "loss: 2.287193  [ 7600/ 8483]\n",
            "loss: 2.275803  [ 7800/ 8483]\n",
            "loss: 2.273232  [ 8000/ 8483]\n",
            "loss: 2.279799  [ 8200/ 8483]\n",
            "loss: 2.264836  [ 3486/ 8483]\n",
            "For epoch 3 the test accuracy over a batch of the test set is 17 %\n",
            "loss: 2.262558  [    0/ 8483]\n",
            "loss: 2.273793  [  200/ 8483]\n",
            "loss: 2.259338  [  400/ 8483]\n",
            "loss: 2.256452  [  600/ 8483]\n",
            "loss: 2.240613  [  800/ 8483]\n",
            "loss: 2.217614  [ 1000/ 8483]\n",
            "loss: 2.270403  [ 1200/ 8483]\n",
            "loss: 2.310827  [ 1400/ 8483]\n",
            "loss: 2.242120  [ 1600/ 8483]\n",
            "loss: 2.283924  [ 1800/ 8483]\n",
            "loss: 2.244292  [ 2000/ 8483]\n",
            "loss: 2.244214  [ 2200/ 8483]\n",
            "loss: 2.280199  [ 2400/ 8483]\n",
            "loss: 2.237009  [ 2600/ 8483]\n",
            "loss: 2.209940  [ 2800/ 8483]\n",
            "loss: 2.202039  [ 3000/ 8483]\n",
            "loss: 2.203669  [ 3200/ 8483]\n",
            "loss: 2.199895  [ 3400/ 8483]\n",
            "loss: 2.183274  [ 3600/ 8483]\n",
            "loss: 2.261626  [ 3800/ 8483]\n",
            "loss: 2.086247  [ 4000/ 8483]\n",
            "loss: 2.384304  [ 4200/ 8483]\n",
            "loss: 2.236698  [ 4400/ 8483]\n",
            "loss: 2.247934  [ 4600/ 8483]\n",
            "loss: 2.192069  [ 4800/ 8483]\n",
            "loss: 2.233740  [ 5000/ 8483]\n",
            "loss: 2.200954  [ 5200/ 8483]\n",
            "loss: 2.240991  [ 5400/ 8483]\n",
            "loss: 2.190273  [ 5600/ 8483]\n",
            "loss: 2.456187  [ 5800/ 8483]\n",
            "loss: 3.021337  [ 6000/ 8483]\n",
            "loss: 2.374577  [ 6200/ 8483]\n",
            "loss: 2.193393  [ 6400/ 8483]\n",
            "loss: 2.391864  [ 6600/ 8483]\n",
            "loss: 2.194591  [ 6800/ 8483]\n",
            "loss: 2.274239  [ 7000/ 8483]\n",
            "loss: 2.231205  [ 7200/ 8483]\n",
            "loss: 2.213666  [ 7400/ 8483]\n",
            "loss: 2.183806  [ 7600/ 8483]\n",
            "loss: 2.319065  [ 7800/ 8483]\n",
            "loss: 2.155531  [ 8000/ 8483]\n",
            "loss: 2.194496  [ 8200/ 8483]\n",
            "loss: 2.130796  [ 3486/ 8483]\n",
            "For epoch 4 the test accuracy over a batch of the test set is 22 %\n",
            "loss: 2.157535  [    0/ 8483]\n",
            "loss: 2.142547  [  200/ 8483]\n",
            "loss: 2.063449  [  400/ 8483]\n",
            "loss: 2.116652  [  600/ 8483]\n",
            "loss: 2.051707  [  800/ 8483]\n",
            "loss: 2.103655  [ 1000/ 8483]\n",
            "loss: 2.407546  [ 1200/ 8483]\n",
            "loss: 2.208200  [ 1400/ 8483]\n",
            "loss: 2.235389  [ 1600/ 8483]\n",
            "loss: 2.212871  [ 1800/ 8483]\n",
            "loss: 2.242567  [ 2000/ 8483]\n",
            "loss: 2.426569  [ 2200/ 8483]\n",
            "loss: 2.309103  [ 2400/ 8483]\n",
            "loss: 2.253055  [ 2600/ 8483]\n",
            "loss: 2.238215  [ 2800/ 8483]\n",
            "loss: 2.252154  [ 3000/ 8483]\n",
            "loss: 2.222202  [ 3200/ 8483]\n",
            "loss: 2.259707  [ 3400/ 8483]\n",
            "loss: 2.229839  [ 3600/ 8483]\n",
            "loss: 2.231260  [ 3800/ 8483]\n",
            "loss: 2.204266  [ 4000/ 8483]\n",
            "loss: 2.170401  [ 4200/ 8483]\n",
            "loss: 2.135163  [ 4400/ 8483]\n",
            "loss: 2.093518  [ 4600/ 8483]\n",
            "loss: 2.324355  [ 4800/ 8483]\n",
            "loss: 2.062872  [ 5000/ 8483]\n",
            "loss: 2.165287  [ 5200/ 8483]\n",
            "loss: 2.871700  [ 5400/ 8483]\n",
            "loss: 2.205955  [ 5600/ 8483]\n",
            "loss: 2.299993  [ 5800/ 8483]\n",
            "loss: 2.389650  [ 6000/ 8483]\n",
            "loss: 2.268792  [ 6200/ 8483]\n",
            "loss: 2.324172  [ 6400/ 8483]\n",
            "loss: 2.238311  [ 6600/ 8483]\n",
            "loss: 2.407840  [ 6800/ 8483]\n",
            "loss: 2.270812  [ 7000/ 8483]\n",
            "loss: 2.291662  [ 7200/ 8483]\n",
            "loss: 2.253462  [ 7400/ 8483]\n",
            "loss: 2.631883  [ 7600/ 8483]\n",
            "loss: 2.329227  [ 7800/ 8483]\n",
            "loss: 3.734325  [ 8000/ 8483]\n",
            "loss: 2.470100  [ 8200/ 8483]\n",
            "loss: 2.273889  [ 3486/ 8483]\n",
            "For epoch 5 the test accuracy over a batch of the test set is 12 %\n",
            "loss: 2.270767  [    0/ 8483]\n",
            "loss: 2.766850  [  200/ 8483]\n",
            "loss: 2.308936  [  400/ 8483]\n",
            "loss: 4.650178  [  600/ 8483]\n",
            "loss: 2.480089  [  800/ 8483]\n",
            "loss: 2.339237  [ 1000/ 8483]\n",
            "loss: 2.344934  [ 1200/ 8483]\n",
            "loss: 2.307381  [ 1400/ 8483]\n",
            "loss: 2.283471  [ 1600/ 8483]\n",
            "loss: 7.020962  [ 1800/ 8483]\n",
            "loss: 2.374263  [ 2000/ 8483]\n",
            "loss: 2.308956  [ 2200/ 8483]\n",
            "loss: 2.354852  [ 2400/ 8483]\n",
            "loss: 2.290242  [ 2600/ 8483]\n",
            "loss: 2.320198  [ 2800/ 8483]\n",
            "loss: 2.304505  [ 3000/ 8483]\n",
            "loss: 2.298894  [ 3200/ 8483]\n",
            "loss: 2.287848  [ 3400/ 8483]\n",
            "loss: 2.288676  [ 3600/ 8483]\n",
            "loss: 2.293403  [ 3800/ 8483]\n",
            "loss: 2.287832  [ 4000/ 8483]\n",
            "loss: 2.435790  [ 4200/ 8483]\n",
            "loss: 2.338318  [ 4400/ 8483]\n",
            "loss: 2.322139  [ 4600/ 8483]\n",
            "loss: 2.301975  [ 4800/ 8483]\n",
            "loss: 2.300213  [ 5000/ 8483]\n",
            "loss: 2.315265  [ 5200/ 8483]\n",
            "loss: 2.290560  [ 5400/ 8483]\n",
            "loss: 2.302719  [ 5600/ 8483]\n",
            "loss: 2.310740  [ 5800/ 8483]\n",
            "loss: 2.287270  [ 6000/ 8483]\n",
            "loss: 2.281183  [ 6200/ 8483]\n",
            "loss: 2.272987  [ 6400/ 8483]\n",
            "loss: 2.243856  [ 6600/ 8483]\n",
            "loss: 2.390393  [ 6800/ 8483]\n",
            "loss: 2.350508  [ 7000/ 8483]\n",
            "loss: 2.317373  [ 7200/ 8483]\n",
            "loss: 2.245603  [ 7400/ 8483]\n",
            "loss: 2.421967  [ 7600/ 8483]\n",
            "loss: 2.300463  [ 7800/ 8483]\n",
            "loss: 2.297343  [ 8000/ 8483]\n",
            "loss: 2.296535  [ 8200/ 8483]\n",
            "loss: 2.298443  [ 3486/ 8483]\n",
            "For epoch 6 the test accuracy over a batch of the test set is 9 %\n",
            "loss: 2.337230  [    0/ 8483]\n",
            "loss: 2.287333  [  200/ 8483]\n",
            "loss: 2.285775  [  400/ 8483]\n",
            "loss: 2.286914  [  600/ 8483]\n",
            "loss: 2.265329  [  800/ 8483]\n",
            "loss: 2.314609  [ 1000/ 8483]\n",
            "loss: 2.269731  [ 1200/ 8483]\n",
            "loss: 2.249199  [ 1400/ 8483]\n",
            "loss: 2.187493  [ 1600/ 8483]\n",
            "loss: 2.290413  [ 1800/ 8483]\n",
            "loss: 2.153306  [ 2000/ 8483]\n",
            "loss: 2.592811  [ 2200/ 8483]\n",
            "loss: 2.138480  [ 2400/ 8483]\n",
            "loss: 2.211513  [ 2600/ 8483]\n",
            "loss: 2.168658  [ 2800/ 8483]\n",
            "loss: 2.140734  [ 3000/ 8483]\n",
            "loss: 2.058993  [ 3200/ 8483]\n",
            "loss: 1.952760  [ 3400/ 8483]\n",
            "loss: 2.201133  [ 3600/ 8483]\n",
            "loss: 2.359083  [ 3800/ 8483]\n",
            "loss: 2.242976  [ 4000/ 8483]\n",
            "loss: 2.145318  [ 4200/ 8483]\n",
            "loss: 2.227188  [ 4400/ 8483]\n",
            "loss: 2.112175  [ 4600/ 8483]\n",
            "loss: 2.106230  [ 4800/ 8483]\n",
            "loss: 2.055116  [ 5000/ 8483]\n",
            "loss: 2.325406  [ 5200/ 8483]\n",
            "loss: 2.175120  [ 5400/ 8483]\n",
            "loss: 2.182531  [ 5600/ 8483]\n",
            "loss: 2.266671  [ 5800/ 8483]\n",
            "loss: 2.237864  [ 6000/ 8483]\n",
            "loss: 2.224919  [ 6200/ 8483]\n",
            "loss: 2.318763  [ 6400/ 8483]\n",
            "loss: 2.229174  [ 6600/ 8483]\n",
            "loss: 2.286474  [ 6800/ 8483]\n",
            "loss: 2.248583  [ 7000/ 8483]\n",
            "loss: 2.265908  [ 7200/ 8483]\n",
            "loss: 2.241089  [ 7400/ 8483]\n",
            "loss: 2.241374  [ 7600/ 8483]\n",
            "loss: 2.226279  [ 7800/ 8483]\n",
            "loss: 2.232386  [ 8000/ 8483]\n",
            "loss: 2.224532  [ 8200/ 8483]\n",
            "loss: 2.174400  [ 3486/ 8483]\n",
            "For epoch 7 the test accuracy over a batch of the test set is 21 %\n",
            "loss: 2.222617  [    0/ 8483]\n",
            "loss: 2.193614  [  200/ 8483]\n",
            "loss: 2.139833  [  400/ 8483]\n",
            "loss: 2.192190  [  600/ 8483]\n",
            "loss: 2.124422  [  800/ 8483]\n",
            "loss: 2.125589  [ 1000/ 8483]\n",
            "loss: 2.113009  [ 1200/ 8483]\n",
            "loss: 2.043503  [ 1400/ 8483]\n",
            "loss: 2.016548  [ 1600/ 8483]\n",
            "loss: 2.040682  [ 1800/ 8483]\n",
            "loss: 2.026155  [ 2000/ 8483]\n",
            "loss: 1.948924  [ 2200/ 8483]\n",
            "loss: 2.009012  [ 2400/ 8483]\n",
            "loss: 2.392760  [ 2600/ 8483]\n",
            "loss: 2.002745  [ 2800/ 8483]\n",
            "loss: 2.000447  [ 3000/ 8483]\n",
            "loss: 2.111601  [ 3200/ 8483]\n",
            "loss: 2.027999  [ 3400/ 8483]\n",
            "loss: 1.988882  [ 3600/ 8483]\n",
            "loss: 2.032451  [ 3800/ 8483]\n",
            "loss: 1.990432  [ 4000/ 8483]\n",
            "loss: 1.926861  [ 4200/ 8483]\n",
            "loss: 1.976635  [ 4400/ 8483]\n",
            "loss: 1.950883  [ 4600/ 8483]\n",
            "loss: 1.921540  [ 4800/ 8483]\n",
            "loss: 1.924944  [ 5000/ 8483]\n",
            "loss: 1.945014  [ 5200/ 8483]\n",
            "loss: 1.863838  [ 5400/ 8483]\n",
            "loss: 1.915619  [ 5600/ 8483]\n",
            "loss: 1.817776  [ 5800/ 8483]\n",
            "loss: 1.849407  [ 6000/ 8483]\n",
            "loss: 1.777324  [ 6200/ 8483]\n",
            "loss: 1.796551  [ 6400/ 8483]\n",
            "loss: 1.748775  [ 6600/ 8483]\n",
            "loss: 1.845121  [ 6800/ 8483]\n",
            "loss: 1.775562  [ 7000/ 8483]\n",
            "loss: 1.943604  [ 7200/ 8483]\n",
            "loss: 1.841692  [ 7400/ 8483]\n",
            "loss: 1.625869  [ 7600/ 8483]\n",
            "loss: 1.691425  [ 7800/ 8483]\n",
            "loss: 1.739920  [ 8000/ 8483]\n",
            "loss: 1.783985  [ 8200/ 8483]\n",
            "loss: 1.729241  [ 3486/ 8483]\n",
            "For epoch 8 the test accuracy over a batch of the test set is 36 %\n",
            "loss: 1.751641  [    0/ 8483]\n",
            "loss: 1.984984  [  200/ 8483]\n",
            "loss: 1.783263  [  400/ 8483]\n",
            "loss: 1.804860  [  600/ 8483]\n",
            "loss: 1.813683  [  800/ 8483]\n",
            "loss: 1.780159  [ 1000/ 8483]\n",
            "loss: 1.762709  [ 1200/ 8483]\n",
            "loss: 1.864838  [ 1400/ 8483]\n",
            "loss: 1.699782  [ 1600/ 8483]\n",
            "loss: 1.728915  [ 1800/ 8483]\n",
            "loss: 1.807780  [ 2000/ 8483]\n",
            "loss: 1.586541  [ 2200/ 8483]\n",
            "loss: 1.746834  [ 2400/ 8483]\n",
            "loss: 1.882244  [ 2600/ 8483]\n",
            "loss: 1.695011  [ 2800/ 8483]\n",
            "loss: 1.827108  [ 3000/ 8483]\n",
            "loss: 1.619910  [ 3200/ 8483]\n",
            "loss: 1.787576  [ 3400/ 8483]\n",
            "loss: 1.711872  [ 3600/ 8483]\n",
            "loss: 1.706633  [ 3800/ 8483]\n",
            "loss: 1.677935  [ 4000/ 8483]\n",
            "loss: 1.642272  [ 4200/ 8483]\n",
            "loss: 1.619048  [ 4400/ 8483]\n",
            "loss: 1.578929  [ 4600/ 8483]\n",
            "loss: 1.652408  [ 4800/ 8483]\n",
            "loss: 1.637088  [ 5000/ 8483]\n",
            "loss: 1.730314  [ 5200/ 8483]\n",
            "loss: 1.568759  [ 5400/ 8483]\n",
            "loss: 1.594570  [ 5600/ 8483]\n",
            "loss: 1.564051  [ 5800/ 8483]\n",
            "loss: 1.591385  [ 6000/ 8483]\n",
            "loss: 1.491314  [ 6200/ 8483]\n",
            "loss: 1.523186  [ 6400/ 8483]\n",
            "loss: 1.451216  [ 6600/ 8483]\n",
            "loss: 1.491115  [ 6800/ 8483]\n",
            "loss: 1.430093  [ 7000/ 8483]\n",
            "loss: 1.721463  [ 7200/ 8483]\n",
            "loss: 1.612979  [ 7400/ 8483]\n",
            "loss: 1.534926  [ 7600/ 8483]\n",
            "loss: 1.478863  [ 7800/ 8483]\n",
            "loss: 1.598383  [ 8000/ 8483]\n",
            "loss: 1.699919  [ 8200/ 8483]\n",
            "loss: 1.502908  [ 3486/ 8483]\n",
            "For epoch 9 the test accuracy over a batch of the test set is 44 %\n",
            "loss: 1.399514  [    0/ 8483]\n",
            "loss: 1.535351  [  200/ 8483]\n",
            "loss: 1.495777  [  400/ 8483]\n",
            "loss: 1.622434  [  600/ 8483]\n",
            "loss: 1.552168  [  800/ 8483]\n",
            "loss: 1.431824  [ 1000/ 8483]\n",
            "loss: 1.614797  [ 1200/ 8483]\n",
            "loss: 1.406047  [ 1400/ 8483]\n",
            "loss: 1.516409  [ 1600/ 8483]\n",
            "loss: 1.415714  [ 1800/ 8483]\n",
            "loss: 1.471884  [ 2000/ 8483]\n",
            "loss: 1.614793  [ 2200/ 8483]\n",
            "loss: 1.551094  [ 2400/ 8483]\n",
            "loss: 1.754964  [ 2600/ 8483]\n",
            "loss: 1.698655  [ 2800/ 8483]\n",
            "loss: 1.603961  [ 3000/ 8483]\n",
            "loss: 1.722443  [ 3200/ 8483]\n",
            "loss: 1.482145  [ 3400/ 8483]\n",
            "loss: 1.742805  [ 3600/ 8483]\n",
            "loss: 1.599966  [ 3800/ 8483]\n",
            "loss: 1.493149  [ 4000/ 8483]\n",
            "loss: 1.684723  [ 4200/ 8483]\n",
            "loss: 1.523739  [ 4400/ 8483]\n",
            "loss: 1.641092  [ 4600/ 8483]\n",
            "loss: 1.491570  [ 4800/ 8483]\n",
            "loss: 1.501842  [ 5000/ 8483]\n",
            "loss: 1.642177  [ 5200/ 8483]\n",
            "loss: 1.566061  [ 5400/ 8483]\n",
            "loss: 1.495327  [ 5600/ 8483]\n",
            "loss: 1.571689  [ 5800/ 8483]\n",
            "loss: 1.343912  [ 6000/ 8483]\n",
            "loss: 1.372374  [ 6200/ 8483]\n",
            "loss: 1.410640  [ 6400/ 8483]\n",
            "loss: 1.473094  [ 6600/ 8483]\n",
            "loss: 1.380790  [ 6800/ 8483]\n",
            "loss: 1.334819  [ 7000/ 8483]\n",
            "loss: 1.304206  [ 7200/ 8483]\n",
            "loss: 1.311931  [ 7400/ 8483]\n",
            "loss: 1.504417  [ 7600/ 8483]\n",
            "loss: 1.499853  [ 7800/ 8483]\n",
            "loss: 1.225795  [ 8000/ 8483]\n",
            "loss: 1.299510  [ 8200/ 8483]\n",
            "loss: 1.413875  [ 3486/ 8483]\n",
            "For epoch 10 the test accuracy over a batch of the test set is 44 %\n",
            "loss: 1.481839  [    0/ 8483]\n",
            "loss: 1.297514  [  200/ 8483]\n",
            "loss: 1.537015  [  400/ 8483]\n",
            "loss: 1.301748  [  600/ 8483]\n",
            "loss: 1.627200  [  800/ 8483]\n",
            "loss: 1.339404  [ 1000/ 8483]\n",
            "loss: 1.341734  [ 1200/ 8483]\n",
            "loss: 1.445666  [ 1400/ 8483]\n",
            "loss: 1.348756  [ 1600/ 8483]\n",
            "loss: 1.339402  [ 1800/ 8483]\n",
            "loss: 1.408539  [ 2000/ 8483]\n",
            "loss: 1.355085  [ 2200/ 8483]\n",
            "loss: 1.310044  [ 2400/ 8483]\n",
            "loss: 1.189369  [ 2600/ 8483]\n",
            "loss: 1.208188  [ 2800/ 8483]\n",
            "loss: 1.338671  [ 3000/ 8483]\n",
            "loss: 1.423985  [ 3200/ 8483]\n",
            "loss: 1.251164  [ 3400/ 8483]\n",
            "loss: 1.182751  [ 3600/ 8483]\n",
            "loss: 1.304367  [ 3800/ 8483]\n",
            "loss: 1.338099  [ 4000/ 8483]\n",
            "loss: 1.296710  [ 4200/ 8483]\n",
            "loss: 1.435570  [ 4400/ 8483]\n",
            "loss: 1.282519  [ 4600/ 8483]\n",
            "loss: 1.141754  [ 4800/ 8483]\n",
            "loss: 1.174761  [ 5000/ 8483]\n",
            "loss: 1.236010  [ 5200/ 8483]\n",
            "loss: 1.359553  [ 5400/ 8483]\n",
            "loss: 1.389208  [ 5600/ 8483]\n",
            "loss: 1.325966  [ 5800/ 8483]\n",
            "loss: 1.164059  [ 6000/ 8483]\n",
            "loss: 1.262832  [ 6200/ 8483]\n",
            "loss: 1.328879  [ 6400/ 8483]\n",
            "loss: 1.335579  [ 6600/ 8483]\n",
            "loss: 1.279417  [ 6800/ 8483]\n",
            "loss: 1.300761  [ 7000/ 8483]\n",
            "loss: 1.163439  [ 7200/ 8483]\n",
            "loss: 1.285906  [ 7400/ 8483]\n",
            "loss: 1.306890  [ 7600/ 8483]\n",
            "loss: 1.255885  [ 7800/ 8483]\n",
            "loss: 1.085449  [ 8000/ 8483]\n",
            "loss: 1.283786  [ 8200/ 8483]\n",
            "loss: 1.497523  [ 3486/ 8483]\n",
            "For epoch 11 the test accuracy over a batch of the test set is 53 %\n",
            "loss: 1.281777  [    0/ 8483]\n",
            "loss: 1.140973  [  200/ 8483]\n",
            "loss: 1.245261  [  400/ 8483]\n",
            "loss: 1.156279  [  600/ 8483]\n",
            "loss: 1.358801  [  800/ 8483]\n",
            "loss: 1.150049  [ 1000/ 8483]\n",
            "loss: 1.112422  [ 1200/ 8483]\n",
            "loss: 1.234214  [ 1400/ 8483]\n",
            "loss: 1.127231  [ 1600/ 8483]\n",
            "loss: 1.154388  [ 1800/ 8483]\n",
            "loss: 1.144439  [ 2000/ 8483]\n",
            "loss: 1.187905  [ 2200/ 8483]\n",
            "loss: 1.375110  [ 2400/ 8483]\n",
            "loss: 1.418348  [ 2600/ 8483]\n",
            "loss: 1.272963  [ 2800/ 8483]\n",
            "loss: 1.197707  [ 3000/ 8483]\n",
            "loss: 1.066438  [ 3200/ 8483]\n",
            "loss: 1.096092  [ 3400/ 8483]\n",
            "loss: 1.287812  [ 3600/ 8483]\n",
            "loss: 1.045806  [ 3800/ 8483]\n",
            "loss: 1.177063  [ 4000/ 8483]\n",
            "loss: 1.076488  [ 4200/ 8483]\n",
            "loss: 1.113387  [ 4400/ 8483]\n",
            "loss: 1.136039  [ 4600/ 8483]\n",
            "loss: 1.306524  [ 4800/ 8483]\n",
            "loss: 0.940505  [ 5000/ 8483]\n",
            "loss: 1.026927  [ 5200/ 8483]\n",
            "loss: 1.006477  [ 5400/ 8483]\n",
            "loss: 1.274910  [ 5600/ 8483]\n",
            "loss: 1.099777  [ 5800/ 8483]\n",
            "loss: 1.115572  [ 6000/ 8483]\n",
            "loss: 1.055286  [ 6200/ 8483]\n",
            "loss: 0.980603  [ 6400/ 8483]\n",
            "loss: 1.024103  [ 6600/ 8483]\n",
            "loss: 1.069968  [ 6800/ 8483]\n",
            "loss: 1.102473  [ 7000/ 8483]\n",
            "loss: 1.014542  [ 7200/ 8483]\n",
            "loss: 1.220634  [ 7400/ 8483]\n",
            "loss: 1.191185  [ 7600/ 8483]\n",
            "loss: 1.139943  [ 7800/ 8483]\n",
            "loss: 1.158846  [ 8000/ 8483]\n",
            "loss: 1.108934  [ 8200/ 8483]\n",
            "loss: 0.953306  [ 3486/ 8483]\n",
            "For epoch 12 the test accuracy over a batch of the test set is 55 %\n",
            "loss: 1.236211  [    0/ 8483]\n",
            "loss: 1.096742  [  200/ 8483]\n",
            "loss: 1.037280  [  400/ 8483]\n",
            "loss: 0.995089  [  600/ 8483]\n",
            "loss: 1.157498  [  800/ 8483]\n",
            "loss: 1.001010  [ 1000/ 8483]\n",
            "loss: 1.098962  [ 1200/ 8483]\n",
            "loss: 0.956310  [ 1400/ 8483]\n",
            "loss: 1.025724  [ 1600/ 8483]\n",
            "loss: 1.140333  [ 1800/ 8483]\n",
            "loss: 0.941980  [ 2000/ 8483]\n",
            "loss: 1.057359  [ 2200/ 8483]\n",
            "loss: 0.966371  [ 2400/ 8483]\n",
            "loss: 0.859588  [ 2600/ 8483]\n",
            "loss: 1.098550  [ 2800/ 8483]\n",
            "loss: 0.985476  [ 3000/ 8483]\n",
            "loss: 0.992169  [ 3200/ 8483]\n",
            "loss: 0.984757  [ 3400/ 8483]\n",
            "loss: 1.007361  [ 3600/ 8483]\n",
            "loss: 0.969041  [ 3800/ 8483]\n",
            "loss: 1.004330  [ 4000/ 8483]\n",
            "loss: 1.168449  [ 4200/ 8483]\n",
            "loss: 0.957808  [ 4400/ 8483]\n",
            "loss: 1.016569  [ 4600/ 8483]\n",
            "loss: 1.013930  [ 4800/ 8483]\n",
            "loss: 0.947537  [ 5000/ 8483]\n",
            "loss: 1.059198  [ 5200/ 8483]\n",
            "loss: 0.853972  [ 5400/ 8483]\n",
            "loss: 0.782927  [ 5600/ 8483]\n",
            "loss: 1.095522  [ 5800/ 8483]\n",
            "loss: 0.958020  [ 6000/ 8483]\n",
            "loss: 0.919744  [ 6200/ 8483]\n",
            "loss: 1.130610  [ 6400/ 8483]\n",
            "loss: 0.937795  [ 6600/ 8483]\n",
            "loss: 1.108895  [ 6800/ 8483]\n",
            "loss: 0.883128  [ 7000/ 8483]\n",
            "loss: 0.895942  [ 7200/ 8483]\n",
            "loss: 0.896002  [ 7400/ 8483]\n",
            "loss: 1.013301  [ 7600/ 8483]\n",
            "loss: 0.975559  [ 7800/ 8483]\n",
            "loss: 0.901092  [ 8000/ 8483]\n",
            "loss: 0.967123  [ 8200/ 8483]\n",
            "loss: 1.078295  [ 3486/ 8483]\n",
            "For epoch 13 the test accuracy over a batch of the test set is 64 %\n",
            "loss: 0.793916  [    0/ 8483]\n",
            "loss: 0.967021  [  200/ 8483]\n",
            "loss: 0.931522  [  400/ 8483]\n",
            "loss: 1.023207  [  600/ 8483]\n",
            "loss: 0.872974  [  800/ 8483]\n",
            "loss: 0.786511  [ 1000/ 8483]\n",
            "loss: 0.986385  [ 1200/ 8483]\n",
            "loss: 0.713364  [ 1400/ 8483]\n",
            "loss: 0.894932  [ 1600/ 8483]\n",
            "loss: 0.864203  [ 1800/ 8483]\n",
            "loss: 0.808781  [ 2000/ 8483]\n",
            "loss: 0.827681  [ 2200/ 8483]\n",
            "loss: 0.919007  [ 2400/ 8483]\n",
            "loss: 0.953701  [ 2600/ 8483]\n",
            "loss: 0.922561  [ 2800/ 8483]\n",
            "loss: 0.905024  [ 3000/ 8483]\n",
            "loss: 0.883019  [ 3200/ 8483]\n",
            "loss: 0.838221  [ 3400/ 8483]\n",
            "loss: 0.873537  [ 3600/ 8483]\n",
            "loss: 0.862438  [ 3800/ 8483]\n",
            "loss: 0.968163  [ 4000/ 8483]\n",
            "loss: 0.953210  [ 4200/ 8483]\n",
            "loss: 0.834895  [ 4400/ 8483]\n",
            "loss: 0.950149  [ 4600/ 8483]\n",
            "loss: 0.906267  [ 4800/ 8483]\n",
            "loss: 0.828256  [ 5000/ 8483]\n",
            "loss: 0.826273  [ 5200/ 8483]\n",
            "loss: 0.900525  [ 5400/ 8483]\n",
            "loss: 0.854251  [ 5600/ 8483]\n",
            "loss: 0.884231  [ 5800/ 8483]\n",
            "loss: 0.808077  [ 6000/ 8483]\n",
            "loss: 0.795202  [ 6200/ 8483]\n",
            "loss: 0.826537  [ 6400/ 8483]\n",
            "loss: 0.837970  [ 6600/ 8483]\n",
            "loss: 0.803923  [ 6800/ 8483]\n",
            "loss: 0.884111  [ 7000/ 8483]\n",
            "loss: 0.651278  [ 7200/ 8483]\n",
            "loss: 0.748549  [ 7400/ 8483]\n",
            "loss: 0.734478  [ 7600/ 8483]\n",
            "loss: 0.847693  [ 7800/ 8483]\n",
            "loss: 0.772754  [ 8000/ 8483]\n",
            "loss: 0.838291  [ 8200/ 8483]\n",
            "loss: 0.814672  [ 3486/ 8483]\n",
            "For epoch 14 the test accuracy over a batch of the test set is 66 %\n",
            "loss: 0.777156  [    0/ 8483]\n",
            "loss: 0.895573  [  200/ 8483]\n",
            "loss: 0.877806  [  400/ 8483]\n",
            "loss: 0.900525  [  600/ 8483]\n",
            "loss: 0.822088  [  800/ 8483]\n",
            "loss: 0.784404  [ 1000/ 8483]\n",
            "loss: 0.873998  [ 1200/ 8483]\n",
            "loss: 0.870383  [ 1400/ 8483]\n",
            "loss: 0.979422  [ 1600/ 8483]\n",
            "loss: 0.910494  [ 1800/ 8483]\n",
            "loss: 0.803652  [ 2000/ 8483]\n",
            "loss: 1.057353  [ 2200/ 8483]\n",
            "loss: 0.802337  [ 2400/ 8483]\n",
            "loss: 0.996696  [ 2600/ 8483]\n",
            "loss: 0.923676  [ 2800/ 8483]\n",
            "loss: 0.741630  [ 3000/ 8483]\n",
            "loss: 0.880995  [ 3200/ 8483]\n",
            "loss: 0.979388  [ 3400/ 8483]\n",
            "loss: 0.743026  [ 3600/ 8483]\n",
            "loss: 0.731251  [ 3800/ 8483]\n",
            "loss: 0.752694  [ 4000/ 8483]\n",
            "loss: 0.917483  [ 4200/ 8483]\n",
            "loss: 0.778863  [ 4400/ 8483]\n",
            "loss: 0.917953  [ 4600/ 8483]\n",
            "loss: 0.806006  [ 4800/ 8483]\n",
            "loss: 0.691435  [ 5000/ 8483]\n",
            "loss: 0.796676  [ 5200/ 8483]\n",
            "loss: 0.680402  [ 5400/ 8483]\n",
            "loss: 0.751302  [ 5600/ 8483]\n",
            "loss: 0.662903  [ 5800/ 8483]\n",
            "loss: 0.708692  [ 6000/ 8483]\n",
            "loss: 0.755122  [ 6200/ 8483]\n",
            "loss: 0.740655  [ 6400/ 8483]\n",
            "loss: 0.856888  [ 6600/ 8483]\n",
            "loss: 0.700445  [ 6800/ 8483]\n",
            "loss: 0.655224  [ 7000/ 8483]\n",
            "loss: 0.957597  [ 7200/ 8483]\n",
            "loss: 0.795178  [ 7400/ 8483]\n",
            "loss: 0.677720  [ 7600/ 8483]\n",
            "loss: 0.855562  [ 7800/ 8483]\n",
            "loss: 0.788374  [ 8000/ 8483]\n",
            "loss: 0.820522  [ 8200/ 8483]\n",
            "loss: 0.641417  [ 3486/ 8483]\n",
            "For epoch 15 the test accuracy over a batch of the test set is 69 %\n",
            "Finished Training\n"
          ]
        }
      ],
      "source": [
        "batch_size = 200\n",
        "train_loader1 = torch.utils.data.DataLoader(traindata, batch_size=batch_size,\n",
        "                                          shuffle=True,num_workers=2)\n",
        "test_loader1 = torch.utils.data.DataLoader(testdata, batch_size=batch_size,\n",
        "                                          shuffle=True,num_workers=2)\n",
        "\n",
        "model1 = models.vgg16(weights=VGG16_Weights.DEFAULT)\n",
        "num_filters = model1.classifier[-1].in_features\n",
        "model1.classifier[-1] = nn.Linear(num_filters,10) #changes output of last dense layer in VGG-16 from ouputing 1000 classes to outputting 10\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model1.parameters(), lr=0.001, weight_decay=0.0001)\n",
        "\n",
        "#Note: First epoch takes much longer to run than subsequent epochs because data has to be loaded in\n",
        "train(model = model1,num_epochs=15,train_loader = train_loader1, test_loader = test_loader1, loss_fn=loss_fn,optimizer=optimizer)\n",
        "print('Finished Training')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading in weights of model that got 75% accuracy for us"
      ],
      "metadata": {
        "id": "riiVrkkYqirD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = models.vgg16(weights=VGG16_Weights.DEFAULT)\n",
        "num_filters = model2.classifier[-1].in_features\n",
        "model2.classifier[-1] = nn.Linear(num_filters,10)\n",
        "\n",
        "modelpath = \"drive/Shareddrives/DL Final Project/DL Final Project/model.pth\"\n",
        "model2.load_state_dict(torch.load(modelpath))\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model2.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QAwTTJ01_mw4",
        "outputId": "1582be33-e796-4100-c857-9a028373a008"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VGG(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU(inplace=True)\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (6): ReLU(inplace=True)\n",
              "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (8): ReLU(inplace=True)\n",
              "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU(inplace=True)\n",
              "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (13): ReLU(inplace=True)\n",
              "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (15): ReLU(inplace=True)\n",
              "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (18): ReLU(inplace=True)\n",
              "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (20): ReLU(inplace=True)\n",
              "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (22): ReLU(inplace=True)\n",
              "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (25): ReLU(inplace=True)\n",
              "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (27): ReLU(inplace=True)\n",
              "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (29): ReLU(inplace=True)\n",
              "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Dropout(p=0.5, inplace=False)\n",
              "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): Dropout(p=0.5, inplace=False)\n",
              "    (6): Linear(in_features=4096, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn.metrics\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "accuracy = testAccuracy(model2,test_loader1)\n",
        "print('the test accuracy over the entire test set is %d %%' % (accuracy))\n",
        "\n",
        "with torch.no_grad():\n",
        "  images,labels = next(iter(test_loader1))\n",
        "  images = Variable(images.to(device))\n",
        "  labels = Variable(labels.to(device))\n",
        "  pred = model2(images)\n",
        "\n",
        "_, predicted = torch.max(pred.data, 1)\n",
        "\n",
        "\n",
        "predicted = [ reverse_genredict[k] for k in predicted.cpu().numpy() ]\n",
        "labeldata = [ reverse_genredict[k] for k in labels.cpu().numpy() ]\n",
        "matrix = sklearn.metrics.confusion_matrix(predicted,labeldata, labels = genre_array)\n",
        "disp = sklearn.metrics.ConfusionMatrixDisplay(matrix,display_labels = genre_array)\n",
        "fig, ax = plt.subplots(figsize=(10,10))\n",
        "disp.plot(ax=ax)\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        },
        "id": "M7obOfavtvo7",
        "outputId": "06bcc24f-ca6f-46f7-af65-4d85b02ecf04"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the test accuracy over the entire test set is 78 %\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x720 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}